{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fatemeh\\AppData\\Local\\Temp\\ipykernel_14900\\379951711.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('train_data.csv')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    #Remove links\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text =re.sub(url_pattern, '', text)\n",
    "\n",
    "    #Removing Numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    #Removing Punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Removing Whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    #Removing Special Characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    #Remove html tags\n",
    "    clean = re.compile('<.*?>')\n",
    "    text= re.sub(clean, '', text)\n",
    "\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_text\"]= train_data['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_text\"]= train_data['cleaned_text'].apply(lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    text = word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_text\"]= train_data['cleaned_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fatemeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fatemeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text =[w for w in tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_text\"]= train_data['cleaned_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatization(text):\n",
    "\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract the lemmas\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_text\"]= train_data['cleaned_text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'warrantee', 'ensure', 'insure', 'guarantee', 'undertake', 'secure', 'vouch', 'assure', 'warrant', 'warranty', 'guaranty'}\n",
      "{'warrant', 'warranty', 'guarantee', 'warrantee'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fatemeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fatemeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "target_words = ['guarantee', 'warranty']\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    print(synonyms)\n",
    "    return synonyms\n",
    "\n",
    "# Find synonyms for the target words\n",
    "target_synonyms = set()\n",
    "for word in target_words:\n",
    "    target_synonyms.update(get_synonyms(word))\n",
    "\n",
    "\n",
    "def check_for_similar_words(tokens):\n",
    "    \n",
    "    # Correct spelling using TextBlob\n",
    "    corrected_tokens = [str(TextBlob(token).correct()) for token in tokens]\n",
    "\n",
    "    # Check for similar words including misspellings\n",
    "    for token in corrected_tokens:\n",
    "        if token in target_words or token in target_synonyms:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"result\"]= train_data['cleaned_text'].apply(check_for_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         asin  overall\n",
      "0  0972683275      5.0\n",
      "1  9806010728      4.0\n",
      "2  B000001OM4      4.0\n",
      "3  B000001OM5      5.0\n",
      "4  B000001ON6      5.0\n"
     ]
    }
   ],
   "source": [
    "# filtering reviews\n",
    "rslt_df = train_data.loc[train_data['result'] == True]\n",
    "\n",
    "# average_score\n",
    "average_score = rslt_df.groupby('asin')['overall'].mean().reset_index()\n",
    "print(average_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "queraproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
