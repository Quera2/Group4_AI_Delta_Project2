{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ffd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input,GlobalMaxPooling1D,Embedding,TextVectorization,LayerNormalization,MultiHeadAttention)\n",
    "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import string\n",
    "from transformers import (BertTokenizerFast,TFBertTokenizer,BertTokenizer,RobertaTokenizerFast,\n",
    "                          DataCollatorWithPadding,TFRobertaForSequenceClassification,TFBertForSequenceClassification,\n",
    "                          TFBertModel,create_optimizer)\n",
    "from keras.models import Model\n",
    "from keras import Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b087528",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12183246",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb5797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking care of the 'vote' column \n",
    "train_data['vote'] = train_data['vote'].str.replace(',', '')\n",
    "train_data['vote'] = pd.to_numeric(train_data['vote'], errors='coerce')\n",
    "median_value = train_data['vote'].median()\n",
    "train_data['vote'].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43144d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['vote'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to few amount of values in column 'style'\n",
    "train_data = train_data.drop('style', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle the nan values in column 'summary'\n",
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337563c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41b288",
   "metadata": {},
   "source": [
    "## First Analysis Task\n",
    "* Plotting the distribution of overall ratings\n",
    "* Checking if the dataset is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4824e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "train_data['overall'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Overall Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c5011",
   "metadata": {},
   "source": [
    "Due to the large number of samples in the class of five stars against other classes, we can conclude that **the dataset is not balanced**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee4fa3",
   "metadata": {},
   "source": [
    "Taking care of this, we are going to balance it by **over sampling**!\n",
    "\n",
    "decreasing the size of the majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "x = train_data.drop('overall', axis=1)\n",
    "y = train_data['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b407b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(x, y)\n",
    "\n",
    "train_data_resampled = pd.concat([X_resampled, y_resampled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d54c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "train_data_resampled['overall'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Overall Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190a309",
   "metadata": {},
   "source": [
    "## Second Analysis Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5718166",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# working on a small part of the dataset due to less computation cost\n",
    "checking_df = train_data_resampled.head(20000)\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # to lowercase\n",
    "    text = ''.join([c for c in text if c.isalpha() or c.isspace()])  # remove nonalphabetic characters\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # remove stop words\n",
    "    return text\n",
    "\n",
    "# apply preprocessing to reviewText\n",
    "checking_df['processed_reviewText'] = checking_df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# categorize reviews\n",
    "checking_df['sentiment'] = checking_df['overall'].apply(lambda x: 'positive' if x in [4, 5] else ('neutral' if x == 3 else 'negative'))\n",
    "\n",
    "# generating word cloud\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# word cloud for each sentiment group\n",
    "positive_text = ' '.join(checking_df[checking_df['sentiment'] == 'positive']['processed_reviewText'])\n",
    "neutral_text = ' '.join(checking_df[checking_df['sentiment'] == 'neutral']['processed_reviewText'])\n",
    "negative_text = ' '.join(checking_df[checking_df['sentiment'] == 'negative']['processed_reviewText'])\n",
    "\n",
    "generate_wordcloud(positive_text, 'Positive Reviews Word Cloud')\n",
    "generate_wordcloud(neutral_text, 'Neutral Reviews Word Cloud')\n",
    "generate_wordcloud(negative_text, 'Negative Reviews Word Cloud')\n",
    "\n",
    "# analyze common words\n",
    "positive_words = Counter(positive_text.split())\n",
    "negative_words = Counter(negative_text.split())\n",
    "\n",
    "common_words = set(positive_words.keys()).intersection(set(negative_words.keys()))\n",
    "print(f\"Common words between positive and negative reviews: {common_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dddef7",
   "metadata": {},
   "source": [
    "**Interpretation of common words**:\n",
    "\n",
    "There might simply be common words both in negative and positive reviews, because there are many words we use that do not carry any specific sentiment by thhemselves, for instance 'chair'. Whoever has used 'chair' in their sentence could have been happy using it or not!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a408cc2",
   "metadata": {},
   "source": [
    "## Third Analysis Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_votes = checking_df.groupby(['reviewerID', 'reviewerName'])['vote'].sum().reset_index()\n",
    "\n",
    "top_reviewers = reviewer_votes.sort_values(by='vote', ascending=False).head(10)\n",
    "\n",
    "# top 10 reviewers\n",
    "print(\"Top 10 Reviewers with Most Useful Comments:\")\n",
    "print(top_reviewers[['reviewerName', 'vote']])\n",
    "\n",
    "# results in another style\n",
    "for idx, row in top_reviewers.iterrows():\n",
    "    print(f\"{row['reviewerName']}: {row['vote']} votes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7f306",
   "metadata": {},
   "source": [
    "## Fourth Analysis Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of each review\n",
    "checking_df['review_length'] = checking_df['reviewText'].apply(len)\n",
    "\n",
    "# histogram of the original review lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(checking_df['review_length'], bins=50, color='blue', edgecolor='black')\n",
    "plt.title('Histogram of Review Lengths (Original)')\n",
    "plt.xlabel('Review Length (number of characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# filtering by considering reviews with length less than the 95th percentile\n",
    "threshold = checking_df['review_length'].quantile(0.95)\n",
    "filtered_df = checking_df[checking_df['review_length'] <= threshold]\n",
    "\n",
    "# histogram of the filtered review lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(filtered_df['review_length'], bins=50, color='green', edgecolor='black')\n",
    "plt.title('Histogram of Review Lengths (Filtered)')\n",
    "plt.xlabel('Review Length (number of characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# analysis of the number of characters\n",
    "mean_length = checking_df['review_length'].mean()\n",
    "max_length = checking_df['review_length'].max()\n",
    "suggested_limit = int(threshold)  # Using the 95th percentile as the suggested limit\n",
    "print(f\"Mean review length: {mean_length}\")\n",
    "print(f\"Max review length: {max_length}\")\n",
    "print(f\"Suggested limit for the number of characters: {suggested_limit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbac0a6",
   "metadata": {},
   "source": [
    "* The histogram of the original review lengths shows a wide range of lengths, with some very long reviews.\n",
    "* After filtering out outliers (above the 95th percentile), the histogram focuses on more typical review lengths.\n",
    "* Also it is good for modeling to limit the number of characters as done here. \n",
    "* This helps in reducing the computational cost without losing significant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537eb42",
   "metadata": {},
   "source": [
    "## Fifth Analysis Task \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3900072",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details = pd.read_csv('title_brand.csv')\n",
    "product_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f77bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_reviews = checking_df[checking_df['overall'] == 5]\n",
    "\n",
    "# five stars \n",
    "counts = five_star_reviews.groupby('asin').size().reset_index(name='five_star_count')\n",
    "\n",
    "# get the top 10\n",
    "top_products = counts.sort_values(by='five_star_count', ascending=False).drop_duplicates().head(10)\n",
    "\n",
    "# merge the top products with product details\n",
    "top_products_details = pd.merge(top_products, product_details, on='asin')\n",
    "top_products_details = top_products_details[['brand', 'title', 'five_star_count']]\n",
    "# dropping duplicates\n",
    "top_products_details = top_products_details.drop_duplicates().reset_index().drop('index', axis=1)\n",
    "# displaying the top ten\n",
    "top_products_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d30a1",
   "metadata": {},
   "source": [
    "## Sixth Analysis Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(checking_df, product_details, on='asin')  \n",
    "\n",
    "# count the number of comments for each brand  \n",
    "brand_comments = df.groupby('brand').size().sort_values(ascending=False).head(10)  \n",
    "\n",
    "# Calculating average score for each brand  \n",
    "brand_avg_score = df.groupby('brand')['overall'].mean()  \n",
    "\n",
    "# average scores for the top 10 brands  \n",
    "top_brands_avg_score = brand_avg_score[brand_comments.index]  \n",
    "\n",
    "# display  \n",
    "result = pd.DataFrame({'Brand': top_brands_avg_score.index, 'Average Score': top_brands_avg_score.values})   \n",
    "result = result.sort_values(by='Average Score', ascending=False).reset_index().drop('index', axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095169a8",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490bd9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eaeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb031e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce718d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365433ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking care of the 'vote' column \n",
    "median_value = test_df['vote'].median()\n",
    "test_df['vote'].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to few amount of values in column 'style'\n",
    "test_df = test_df.drop('style', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf02192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle the nan values in column 'summary'\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_test_df = test_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fe866",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_test_df['sentiment'] = checking_test_df['vote'].apply(lambda x: 'positive' if x in [4, 5] else ('neutral' if x == 3 else 'negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_test_df = checking_test_df[['reviewText', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7547cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16511fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_df2 = checking_df[['reviewText', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df606f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_train_df = checking_df2[:16000]\n",
    "checking_val_df = checking_df2[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_train_df.shape, checking_val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4947f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode & changing the datasets to tensorflow datas\n",
    "  \n",
    "# def prepare_datasets(train_df, val_df, test_df):  \n",
    "#     def create_dataset(df):  \n",
    "#         # Check if the DataFrame is empty  \n",
    "#         if df.empty:  \n",
    "#             raise ValueError(\"The provided DataFrame is empty.\")  \n",
    "\n",
    "#         # Check for NaN values in the 'sentiment' column  \n",
    "#         if df['sentiment'].isnull().sum() > 0:  \n",
    "#             print(\"Dropping NaN values from the sentiment column.\")  \n",
    "#             df = df.dropna(subset=['sentiment'])  \n",
    "        \n",
    "#         # Check if the DataFrame is empty after dropping NaNs  \n",
    "#         if df.empty:  \n",
    "#             raise ValueError(\"The DataFrame is empty after dropping NaN values.\")  \n",
    "\n",
    "#         # Encode the sentiment labels using .loc to avoid SettingWithCopyWarning  \n",
    "#         label_encoder = LabelEncoder()  \n",
    "#         df.loc[:, 'sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])  \n",
    "\n",
    "#         # Create tensors  \n",
    "#         text_tensor = tf.convert_to_tensor(df['reviewText'].values, dtype=tf.string)  \n",
    "#         sentiment_tensor = tf.convert_to_tensor(df['sentiment_encoded'].values, dtype=tf.int64)  \n",
    "\n",
    "#         # Create Dataset  \n",
    "#         dataset = tf.data.Dataset.from_tensor_slices((text_tensor, sentiment_tensor))  \n",
    "\n",
    "#         # Check the size of the dataset before shuffling  \n",
    "#         if len(df) > 0:  \n",
    "#             dataset = dataset.shuffle(buffer_size=len(df))  \n",
    "\n",
    "#         return dataset, label_encoder  \n",
    "\n",
    "#     # Prepare datasets for training, validation, and testing  \n",
    "#     train_dataset, label_encoder = create_dataset(train_df)  \n",
    "#     val_dataset, _ = create_dataset(val_df)  \n",
    "#     test_dataset, _ = create_dataset(test_df)  \n",
    "\n",
    "#     return train_dataset, val_dataset, test_dataset, label_encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df, val_df, test_df, label_encoder = prepare_datasets(checking_train_df, checking_val_df, checking_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b52f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ae0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reviewText,sentiment in train_df.take(2):\n",
    "#   print(reviewText)\n",
    "#   print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8084f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardization(input_data):\n",
    "#     '''\n",
    "#     Input: raw reviews\n",
    "#     output: standardized reviews\n",
    "#     '''\n",
    "#     lowercase=tf.strings.lower(input_data)\n",
    "#     no_tag=tf.strings.regex_replace(lowercase,\"<[^>]+>\",\"\")\n",
    "#     output=tf.strings.regex_replace(no_tag,\"[%s]\"%re.escape(string.punctuation),\"\")\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20634583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE=10000\n",
    "# SEQUENCE_LENGTH=250\n",
    "# EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer=TextVectorization(\n",
    "#     standardize=standardization,\n",
    "#     max_tokens=VOCAB_SIZE,\n",
    "#     output_mode='int',\n",
    "#     output_sequence_length=SEQUENCE_LENGTH\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b38d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc782f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = train_df.map(lambda x, y: x)  \n",
    "# if not training_data:  \n",
    "#     print(\"Training data is empty, please check your input DataFrame.\")  \n",
    "\n",
    "# vectorize_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfe799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorizer(review,label):\n",
    "#     return vectorize_layer(review),label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset=train_df.map(vectorizer)\n",
    "# val_dataset=val_df.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca457b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset=train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# val_dataset=val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bd74d",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def positional_encoding(model_size, SEQUENCE_LENGTH):  \n",
    "#     output = []  \n",
    "#     for pos in range(SEQUENCE_LENGTH):  \n",
    "#         PE = np.zeros((model_size,))  \n",
    "#         for i in range(model_size):  \n",
    "#             if i % 2 == 0:  \n",
    "#                 PE[i] = np.sin(pos / (10000 ** (i / model_size)))  \n",
    "#             else:  \n",
    "#                 PE[i] = np.cos(pos / (10000 ** ((i - 1) / model_size)))  \n",
    "#         output.append(PE)  \n",
    "#     return np.array(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embeddings(Layer):\n",
    "#   def __init__(self, sequence_length, vocab_size, embed_dim,):\n",
    "#     super(Embeddings, self).__init__()\n",
    "#     self.token_embeddings=Embedding(\n",
    "#         input_dim=vocab_size, output_dim=embed_dim)\n",
    "#     self.sequence_length = sequence_length\n",
    "#     self.vocab_size = vocab_size\n",
    "#     self.embed_dim = embed_dim\n",
    "\n",
    "#   def call(self, inputs):\n",
    "#     embedded_tokens = self.token_embeddings(inputs)\n",
    "#     embedded_positions=positional_encoding(\n",
    "#         self.embed_dim,self.sequence_length)\n",
    "#     return embedded_tokens + embedded_positions\n",
    "    \n",
    "#   def compute_mask(self, inputs, mask=None):\n",
    "#     return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "#   def get_config(self): \n",
    "#       config = super().get_config()\n",
    "#       config.update({\n",
    "#         \"sequence_length\": self.sequence_length,\n",
    "#         \"vocab_size\": self.vocab_size,\n",
    "#         \"embed_dim\": self.embed_dim,\n",
    "#       })\n",
    "#       return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3970f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# test_input=tf.constant([[  2, 112,   10,   12,  5,   0,   0,   0,]])\n",
    "\n",
    "# emb=Embeddings(8,20000,256)\n",
    "# emb_out=emb(test_input)\n",
    "# print(emb_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11933bd4",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoder(Layer):\n",
    "#     def __init__(self, embed_dim, dense_dim, num_heads,):\n",
    "#         super(TransformerEncoder, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.dense_dim = dense_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.attention = MultiHeadAttention(\n",
    "#             num_heads=num_heads, key_dim=embed_dim,\n",
    "#         )\n",
    "#         self.dense_proj=tf.keras.Sequential(\n",
    "#             [Dense(dense_dim, activation=\"relu\"),Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm_1 = LayerNormalization()\n",
    "#         self.layernorm_2 = LayerNormalization()\n",
    "#         self.supports_masking = True\n",
    "\n",
    "#     def call(self, inputs, mask=None):\n",
    "#       if mask is not None:\n",
    "#         mask1 = mask[:, :, tf.newaxis]\n",
    "#         mask2 = mask[:,tf.newaxis, :]\n",
    "#         padding_mask = tf.cast(mask1&mask2, dtype=\"int32\")\n",
    "\n",
    "#       attention_output = self.attention(\n",
    "#           query=inputs, key=inputs,value=inputs,attention_mask=padding_mask\n",
    "#       )\n",
    "      \n",
    "#       proj_input = self.layernorm_1(inputs + attention_output)\n",
    "#       proj_output = self.dense_proj(proj_input)\n",
    "#       return self.layernorm_2(proj_input + proj_output)\n",
    "      \n",
    "#     def get_config(self): \n",
    "#       config = super().get_config()\n",
    "#       config.update({\n",
    "#         \"embed_dim\": self.embed_dim,\n",
    "#         \"num_heads\": self.num_heads,\n",
    "#         \"dense_dim\": self.dense_dim,\n",
    "#       })\n",
    "#       return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_outputs = TransformerEncoder(256,2048,2)(emb_out)\n",
    "# print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embeddings2(Layer):  \n",
    "#     def __init__(self, sequence_length, vocab_size, embed_dim):  \n",
    "#         super(Embeddings, self).__init__()  # Ensure the class name matches  \n",
    "#         self.token_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)  \n",
    "#         self.sequence_length = sequence_length  \n",
    "\n",
    "#     def call(self, inputs, mask=None):  \n",
    "#         embedded_tokens = self.token_embeddings(inputs)  \n",
    "#         # Apply your positional encoding logic here as needed  \n",
    "#         embedded_positions = self.positional_encoding(self.token_embeddings.output_dim, self.sequence_length)  \n",
    "#         return embedded_tokens + embedded_positions  \n",
    "\n",
    "#     def compute_mask(self, inputs, mask=None):  \n",
    "#         # Create a mask based on the input values  \n",
    "#         return tf.math.not_equal(inputs, 0)  \n",
    "\n",
    "#     def positional_encoding(self, model_size, sequence_length):  \n",
    "#         output = np.zeros((sequence_length, model_size))  \n",
    "#         for pos in range(sequence_length):  \n",
    "#             for i in range(model_size):  \n",
    "#                 if i % 2 == 0:  \n",
    "#                     output[pos, i] = np.sin(pos / (10000 ** (i / model_size)))  \n",
    "#                 else:  \n",
    "#                     output[pos, i] = np.cos(pos / (10000 ** ((i - 1) / model_size)))  \n",
    "#         return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f73324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embeddings3(Layer):\n",
    "#     def __init__(self, sequence_length, vocab_size, embed_dim,):\n",
    "#         super(Embeddings, self).__init__()\n",
    "#         self.token_embeddings=Embedding(\n",
    "#             input_dim=vocab_size, output_dim=embed_dim)\n",
    "#         self.sequence_length = sequence_length\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         embedded_tokens = self.token_embeddings(inputs)\n",
    "#         embedded_positions = positional_encoding(\n",
    "#             self.embed_dim, self.sequence_length)\n",
    "#         return embedded_tokens + embedded_positions\n",
    "\n",
    "#     def compute_mask(self, inputs, mask=None):\n",
    "#         # Use Lambda layer to wrap TensorFlow operation\n",
    "#         return layers.Lambda(lambda x: tf.math.not_equal(x, 0))(inputs)\n",
    "\n",
    "#     def get_config(self): \n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             \"sequence_length\": self.sequence_length,\n",
    "#             \"vocab_size\": self.vocab_size,\n",
    "#             \"embed_dim\": self.embed_dim,\n",
    "#         })\n",
    "#         return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062fab6",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM=128\n",
    "# D_FF=1024\n",
    "# NUM_HEADS=8\n",
    "# NUM_LAYERS=1\n",
    "# NUM_EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input=Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
    "# x = Embeddings3(SEQUENCE_LENGTH,VOCAB_SIZE,EMBEDDING_DIM)(encoder_input)\n",
    "\n",
    "# for _ in range(NUM_LAYERS):\n",
    "#   x=TransformerEncoder(EMBEDDING_DIM,D_FF,NUM_HEADS)(x)\n",
    "\n",
    "# x = Flatten()(x) \n",
    "# output=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# transformer = tf.keras.Model(\n",
    "#     encoder_input, output, name=\"transformer\"\n",
    "# )\n",
    "# transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd1136",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#               optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bdb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history=transformer.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=val_dataset,\n",
    "#     epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35115905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model_loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'val'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816193c6",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d87849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset=test_df.map(vectorizer)\n",
    "# test_dataset=test_dataset.batch(BATCH_SIZE)\n",
    "# transformer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccc112",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data=tf.data.Dataset.from_tensor_slices([[\"this movie looks very interesting, i love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But atleast this movie recreates this scenes! \"],\n",
    "#                                               [\"very good start, but movie started becoming uninteresting at some point though initially i thought it would have been much more fun. There was too much background noise, so in all i didn't like this movie \"],])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9885aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorizer_test(review):\n",
    "#     return vectorize_layer(review)\n",
    "# test_dataset=test_data.map(vectorizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41d0be",
   "metadata": {},
   "source": [
    "## Bert & Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b04183",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea1124",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b63042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "NUM_EPOCHS = 3\n",
    "MODEL_ID_BERT = \"bert-base-uncased\"\n",
    "MODEL_ID_ROBERTA = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0124ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer for BERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_ID_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56084f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "label_encoder = LabelEncoder()\n",
    "def preprocess_function(examples):  \n",
    "    examples.loc[:, 'sentiment_encoded'] = label_encoder.fit_transform(examples['sentiment']) \n",
    "    return tokenizer(examples[\"reviewText\"], padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize datasets\n",
    "train_data = train_data[['reviewText', 'sentiment_encoded']]\n",
    "test_data = test_data[['reviewText', 'sentiment_encoded']]\n",
    "\n",
    "train_data = train_data.rename(columns={\"reviewText\": \"reviewText\", \"sentiment_encoded\": \"sentiment_encoded\"})\n",
    "test_data = test_data.rename(columns={\"reviewText\": \"reviewText\", \"sentiment_encoded\": \"sentiment_encoded\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the datasets\n",
    "train_data_tokenized = train_data.apply(preprocess_function, axis=1)\n",
    "test_data_tokenized = test_data.apply(preprocess_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d694963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to TensorFlow datasets\n",
    "tf_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(input_ids=train_data_tokenized['input_ids'].tolist(),\n",
    "         token_type_ids=train_data_tokenized['token_type_ids'].tolist(),\n",
    "         attention_mask=train_data_tokenized['attention_mask'].tolist()),\n",
    "    train_data['sentiment_encoded'].tolist()\n",
    ")).batch(BATCH_SIZE).shuffle(buffer_size=100)\n",
    "\n",
    "tf_val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(input_ids=test_data_tokenized['input_ids'].tolist(),\n",
    "         token_type_ids=test_data_tokenized['token_type_ids'].tolist(),\n",
    "         attention_mask=test_data_tokenized['attention_mask'].tolist()),\n",
    "    test_data['sentiment_encoded'].tolist()\n",
    ")).batch(BATCH_SIZE).shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_ID_BERT, num_labels=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d496db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=len(train_data) // BATCH_SIZE * NUM_EPOCHS)\n",
    "model.compile(optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "history = model.fit(tf_train_dataset, validation_data=tf_val_dataset, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea175502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cf890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233da477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with inputs\n",
    "test_inputs = tokenizer([\n",
    "    \"This movie looks very interesting, I love the fact that the actors do a great job in showing how people lived in the 18th century, which wasn't very good at all. But at least this movie recreates these scenes!\",\n",
    "    \"Very good start, but the movie started becoming uninteresting at some point though initially I thought it would have been much more fun. There was too much background noise, but later on towards the middle of the movie, my favorite character got in and he did a great job, so overall.\"\n",
    "], padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b926ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits\n",
    "logits = model(**test_inputs).logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c663c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
